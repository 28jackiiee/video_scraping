#!/usr/bin/env python3
"""
Video Filter for M4 MacBook with MPS acceleration.
Simplified version that avoids problematic imports.
"""

import argparse
import json
import os
import shutil
import sys
from pathlib import Path
from typing import List, Tuple, Dict
import numpy as np

# Import required libraries
try:
    import torch
    import clip
    from PIL import Image
    import cv2
    from sklearn.metrics.pairwise import cosine_similarity
except ImportError as e:
    print(f"Error: Missing required dependencies: {e}")
    print("Run: pip install torch clip-by-openai pillow opencv-python-headless scikit-learn")
    sys.exit(1)


class M4VideoFilter:
    def __init__(self, device=None):
        """Initialize the video filter with CLIP model optimized for M4 MacBook."""
        # Auto-detect best available device, prioritizing MPS for M4 MacBook
        if device:
            self.device = device
        elif torch.backends.mps.is_available():
            self.device = "mps"  # Apple Silicon M4 GPU acceleration
        elif torch.cuda.is_available():
            self.device = "cuda"
        else:
            self.device = "cpu"
        
        print(f"üöÄ M4 Video Filter starting...")
        print(f"Device: {self.device}")
        if self.device == "mps":
            print("üçé Apple M4 GPU acceleration enabled")
        elif self.device == "cuda":
            print("üöÄ NVIDIA GPU acceleration enabled")
        else:
            print("üíª Using CPU (slower processing)")
        
        # Load CLIP model
        try:
            print("Loading CLIP model...")
            self.model, self.preprocess = clip.load("ViT-B/32", device=self.device)
            print("‚úÖ CLIP model loaded successfully")
        except Exception as e:
            print(f"‚ùå Error loading CLIP model: {e}")
            sys.exit(1)
    
    def extract_frames(self, video_path: Path, num_frames: int = 8) -> List[np.ndarray]:
        """Extract evenly spaced frames from a video."""
        print(f"  üìπ Extracting frames from: {video_path.name}")
        cap = cv2.VideoCapture(str(video_path))
        
        if not cap.isOpened():
            print(f"  ‚ö†Ô∏è  Could not open video {video_path}")
            return []
        
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        if total_frames == 0:
            print(f"  ‚ö†Ô∏è  Video {video_path} has no frames")
            cap.release()
            return []
        
        frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)
        frames = []
        
        for frame_idx in frame_indices:
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
            ret, frame = cap.read()
            
            if ret:
                # Convert BGR to RGB
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                frames.append(frame_rgb)
        
        cap.release()
        print(f"  ‚úÖ Extracted {len(frames)} frames")
        return frames
    
    def encode_video(self, video_path: Path) -> np.ndarray:
        """Encode a video into a CLIP embedding by averaging frame embeddings."""
        frames = self.extract_frames(video_path)
        
        if not frames:
            return np.zeros(512)  # Return zero embedding for failed videos
        
        frame_embeddings = []
        
        print(f"  üß† Processing {len(frames)} frames with CLIP...")
        for i, frame in enumerate(frames):
            # Convert numpy array to PIL Image
            pil_image = Image.fromarray(frame)
            
            # Preprocess and encode the frame
            image_input = self.preprocess(pil_image).unsqueeze(0).to(self.device)
            
            with torch.no_grad():
                image_features = self.model.encode_image(image_input)
                image_features = image_features / image_features.norm(dim=-1, keepdim=True)
                frame_embeddings.append(image_features.cpu().numpy())
        
        if frame_embeddings:
            # Average all frame embeddings
            video_embedding = np.mean(frame_embeddings, axis=0)
            print(f"  ‚úÖ Video encoded successfully")
            return video_embedding.flatten()
        else:
            return np.zeros(512)
    
    def encode_text(self, text: str) -> np.ndarray:
        """Encode text into a CLIP embedding."""
        text_input = clip.tokenize([text]).to(self.device)
        
        with torch.no_grad():
            text_features = self.model.encode_text(text_input)
            text_features = text_features / text_features.norm(dim=-1, keepdim=True)
            return text_features.cpu().numpy().flatten()
    
    def calculate_similarity(self, video_embedding: np.ndarray, text_embedding: np.ndarray) -> float:
        """Calculate cosine similarity between video and text embeddings."""
        if video_embedding.shape[0] == 0 or text_embedding.shape[0] == 0:
            return 0.0
        
        video_embedding = video_embedding.reshape(1, -1)
        text_embedding = text_embedding.reshape(1, -1)
        
        similarity = cosine_similarity(video_embedding, text_embedding)[0][0]
        return float(similarity)
    
    def find_videos(self, directory: Path) -> List[Path]:
        """Find all video files in a directory."""
        video_extensions = {'.mp4', '.mov', '.webm', '.avi', '.mkv'}
        video_files = []
        
        for ext in video_extensions:
            video_files.extend(directory.glob(f"*{ext}"))
        
        return sorted(video_files)
    
    def load_metadata(self, directory: Path) -> Dict:
        """Load metadata from a directory."""
        metadata_file = directory / "query_metadata.json"
        
        if not metadata_file.exists():
            return {}
        
        try:
            with open(metadata_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"‚ö†Ô∏è  Warning: Could not load metadata from {metadata_file}: {e}")
            return {}
    
    def filter_videos(self, source_dir: Path, query: str = None, top_k: int = 5) -> List[Tuple[Path, float]]:
        """Filter videos by similarity to query text."""
        print(f"\nüîç Filtering videos in: {source_dir}")
        
        video_results = []
        
        # Process each subdirectory
        for subdir in source_dir.iterdir():
            if not subdir.is_dir():
                continue
            
            print(f"\nüìÅ Processing directory: {subdir.name}")
            
            # Load metadata to get the original query
            metadata = self.load_metadata(subdir)
            
            # Use provided query or fall back to metadata query
            search_query = query
            if not search_query and metadata:
                search_query = metadata.get('original_query', metadata.get('clean_query', ''))
            
            if not search_query:
                print(f"‚ö†Ô∏è  No query found for {subdir.name}, skipping...")
                continue
            
            print(f"üéØ Query: '{search_query}'")
            
            # Encode the text query
            text_embedding = self.encode_text(search_query)
            print(f"‚úÖ Text query encoded")
            
            # Find video files
            video_files = self.find_videos(subdir)
            
            if not video_files:
                print(f"‚ö†Ô∏è  No video files found in {subdir.name}")
                continue
            
            print(f"üé• Found {len(video_files)} video files")
            
            # Process each video
            for video_file in video_files:
                print(f"\nüìπ Processing: {video_file.name}")
                
                # Encode video
                video_embedding = self.encode_video(video_file)
                
                # Calculate similarity
                similarity = self.calculate_similarity(video_embedding, text_embedding)
                
                video_results.append((video_file, similarity))
                print(f"üìä Similarity score: {similarity:.4f}")
        
        # Sort by similarity (highest first)
        video_results.sort(key=lambda x: x[1], reverse=True)
        
        # Return top_k results
        return video_results[:top_k]
    
    def copy_filtered_videos(self, filtered_videos: List[Tuple[Path, float]], output_dir: Path):
        """Copy filtered videos to output directory."""
        output_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"\nüìã Copying {len(filtered_videos)} videos to: {output_dir}")
        
        # Create a results file with similarity scores
        results_data = {
            "filtering_timestamp": str(Path().absolute()),
            "total_videos_filtered": len(filtered_videos),
            "device_used": self.device,
            "videos": []
        }
        
        for i, (video_path, similarity) in enumerate(filtered_videos, 1):
            # Create new filename with rank and similarity
            original_name = video_path.stem
            extension = video_path.suffix
            new_name = f"rank_{i:02d}_sim_{similarity:.4f}_{original_name}{extension}"
            
            output_path = output_dir / new_name
            
            try:
                shutil.copy2(video_path, output_path)
                print(f"  ‚úÖ Copied: {new_name}")
                
                results_data["videos"].append({
                    "rank": i,
                    "original_path": str(video_path),
                    "output_filename": new_name,
                    "similarity_score": similarity,
                    "source_directory": video_path.parent.name
                })
                
            except Exception as e:
                print(f"  ‚ùå Failed to copy {video_path.name}: {e}")
        
        # Save results metadata
        results_file = output_dir / "filtering_results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results_data, f, indent=2, ensure_ascii=False)
        
        print(f"üìä Results saved to: {results_file}")


def main():
    parser = argparse.ArgumentParser(description="Filter videos using CLIP embeddings (M4 MacBook optimized)")
    parser.add_argument(
        "--source_dir", 
        type=str, 
        default="downloads",
        help="Source directory containing video subdirectories (default: downloads)"
    )
    parser.add_argument(
        "--top_k", 
        type=int, 
        default=5,
        help="Number of top matching videos to filter (default: 5)"
    )
    parser.add_argument(
        "--query", 
        type=str,
        help="Text query to match against (if not provided, uses metadata queries)"
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default="filtered",
        help="Output directory for filtered videos (default: filtered)"
    )
    parser.add_argument(
        "--device",
        type=str,
        choices=["auto", "cpu", "cuda", "mps"],
        default="auto",
        help="Device to use: auto (default), cpu, cuda (NVIDIA), or mps (Apple Silicon)"
    )
    
    args = parser.parse_args()
    
    # Validate source directory
    source_path = Path(args.source_dir)
    if not source_path.exists():
        print(f"‚ùå Error: Source directory '{args.source_dir}' does not exist.")
        sys.exit(1)
    
    # Initialize filter
    print("üçé Initializing M4 Video Filter with CLIP...")
    device = None if args.device == "auto" else args.device
    video_filter = M4VideoFilter(device=device)
    
    # Filter videos
    filtered_videos = video_filter.filter_videos(
        source_dir=source_path,
        query=args.query,
        top_k=args.top_k
    )
    
    if not filtered_videos:
        print("‚ùå No videos found matching the criteria.")
        sys.exit(0)
    
    # Copy filtered videos
    output_path = Path(args.output_dir)
    video_filter.copy_filtered_videos(filtered_videos, output_path)
    
    print(f"\nüéâ Filtering complete! {len(filtered_videos)} videos saved to '{args.output_dir}'")
    print(f"üçé M4 MacBook with {'MPS GPU' if video_filter.device == 'mps' else video_filter.device.upper()} acceleration used")
    
    # Display summary
    print("\nüìä Top Results:")
    for i, (video_path, similarity) in enumerate(filtered_videos, 1):
        print(f"  {i}. {video_path.name} (similarity: {similarity:.4f})")


if __name__ == "__main__":
    main() 